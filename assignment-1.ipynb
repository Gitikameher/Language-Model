{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttest: brown/brown.test.txt\n",
      "\ttrain: brown/brown.train.txt\n",
      "\tdev: brown/brown.dev.txt\n",
      "brown  read. train: 39802 dev: 8437 test: 8533\n",
      "\tdev: gutenberg/gutenberg.dev.txt\n",
      "\ttrain: gutenberg/gutenberg.train.txt\n",
      "\ttest: gutenberg/gutenberg.test.txt\n",
      "gutenberg  read. train: 68740 dev: 14729 test: 14826\n",
      "\tdev: reuters/reuters.dev.txt\n",
      "\ttest: reuters/reuters.test.txt\n",
      "\ttrain: reuters/reuters.train.txt\n",
      "reuters  read. train: 38169 dev: 8082 test: 8214\n"
     ]
    }
   ],
   "source": [
    "from data import read_texts\n",
    "from lm import *\n",
    "import numpy as np\n",
    "import math\n",
    "data1 = read_texts(\"data/corpora.tar.gz\", \"brown\")\n",
    "data2=read_texts(\"data/corpora.tar.gz\", \"gutenberg\")\n",
    "data3=read_texts(\"data/corpora.tar.gz\", \"reuters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram=Unigram()\n",
    "unigram.fit_corpus(data1.train)\n",
    "x=list(unigram.model.keys())\n",
    "c=0;modelled_unk=[]\n",
    "for i in x:\n",
    "    if unigram.model[i]==1 and i[0]=='Z':\n",
    "        modelled_unk.append(i)\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pdb; pdb.set_trace()\n",
    "class Trigram(LangModel):\n",
    "    def __init__(self, backoff = 0.000001, modelled_unk=[]):\n",
    "        self.model = dict()\n",
    "        self.lbackoff = log(backoff, 2)\n",
    "        self.modelled_unk=modelled_unk\n",
    "        \n",
    "    def fit_sentence(self, sentence):\n",
    "        sentence.append('END_OF_SENTENCE')\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] in self.modelled_unk:\n",
    "                sentence[i]='UNK'\n",
    "                \n",
    "        for i in range(len(sentence)):\n",
    "            if i==0:\n",
    "                if ('*', '*') in self.model.keys():\n",
    "                    if sentence[i] in self.model[('*','*')].keys():\n",
    "                        self.model[('*','*')][sentence[i]]+=1\n",
    "                    else:\n",
    "                        self.model[('*','*')][sentence[i]]=1\n",
    "                else:\n",
    "                    self.model[('*','*')]={}\n",
    "                    self.model[('*','*')][sentence[i]]=1\n",
    "            elif i==1:\n",
    "                if ('*', sentence[0]) in self.model.keys():\n",
    "                    if sentence[i] in self.model[('*',sentence[0])].keys():\n",
    "                        self.model[('*',sentence[0])][sentence[i]]+=1\n",
    "                    else:\n",
    "                        self.model[('*',sentence[0])][sentence[i]]=1\n",
    "                else:\n",
    "                    self.model[('*',sentence[0])]={}\n",
    "                    self.model[('*',sentence[0])][sentence[i]]=1\n",
    "            else:\n",
    "                if (sentence[i-2], sentence[i-1]) in self.model.keys():\n",
    "                    if sentence[i] in self.model[(sentence[i-2], sentence[i-1])].keys():\n",
    "                        self.model[(sentence[i-2], sentence[i-1])][sentence[i]]+=1\n",
    "                    else:\n",
    "                        self.model[(sentence[i-2], sentence[i-1])][sentence[i]]=1\n",
    "                else:\n",
    "                    self.model[(sentence[i-2], sentence[i-1])]={}\n",
    "                    self.model[(sentence[i-2], sentence[i-1])][sentence[i]]=1\n",
    "    \n",
    "    \n",
    "    def cond_logprob(self, word_tuple, pick):\n",
    "        if word_tuple in self.model.keys():\n",
    "            D=self.model[word_tuple] \n",
    "            return D.get(pick,0)/sum(D.values())\n",
    "        else:\n",
    "            return self.lbackoff   \n",
    "    \n",
    "    def cond_logprob_1(self, word_tuple, previous):\n",
    "        if word_tuple in self.model.keys():\n",
    "            return self.sample(self.model[word_tuple])\n",
    "        else:\n",
    "            return self.lbackoff\n",
    "        \n",
    "    def sample(self, D):\n",
    "        values=np.array(list(D.values()))\n",
    "        prob= values/sum(D.values())\n",
    "        val=np.random.choice(len(D.keys()),1,p=prob)\n",
    "        return list(D.keys())[int(val)]\n",
    "    \n",
    "    def generate_next(self):\n",
    "        i=('*','*');sent=[]\n",
    "        while i[1]!='END_OF_SENTENCE':\n",
    "            sent.append(i[0])\n",
    "            i=(i[1],self.cond_logprob_1(i, 0))\n",
    "            s=' '.join(sent)\n",
    "        return s\n",
    "    def perplexity(self, corpus):\n",
    "        \"\"\"Computes the perplexity of the corpus by the model.\n",
    "\n",
    "        Assumes the model uses an EOS symbol at the end of each sentence.\n",
    "        \"\"\"\n",
    "        return pow(2.0, self.entropy(corpus))\n",
    "\n",
    "    def entropy(self, corpus):\n",
    "        num_words = 0.0\n",
    "        sum_logprob = 0.0\n",
    "        for s in corpus:\n",
    "            num_words += len(s) + 1 # for EOS\n",
    "            sum_logprob += self.logprob_sentence_tri(s)\n",
    "        num_words=num_words-len(self.modelled_unk)+1\n",
    "        return -(1.0/num_words)*(sum_logprob)\n",
    "    \n",
    "    def logprob_sentence_tri(self, sentence):\n",
    "        p=1.0\n",
    "        sentence=['*','*'] + sentence + ['END_OF_SENTENCE']\n",
    "        \n",
    "        for i in range(2, len(sentence)):\n",
    "            p*=self.cond_logprob((sentence[i-2], sentence[i-1]), sentence[i])\n",
    "        return math.log(p,2)\n",
    "    \n",
    "\n",
    "class Bigram(LangModel):\n",
    "    def __init__(self, backoff = 0.000001, modelled_unk=[]):\n",
    "        self.model = dict()\n",
    "        self.lbackoff = log(backoff, 2)\n",
    "        self.modelled_unk=modelled_unk\n",
    "        \n",
    "    def fit_sentence(self, sentence):\n",
    "        sentence.append('END_OF_SENTENCE')\n",
    "        sentence=['*','*']+sentence\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] in self.modelled_unk:\n",
    "                sentence[i]='UNK'\n",
    "                \n",
    "        for i in range(1,len(sentence)):\n",
    "            if sentence[i-1] in self.model.keys():\n",
    "                if sentence[i] in self.model[sentence[i-1]].keys():\n",
    "                    self.model[sentence[i-1]][sentence[i]]+=1\n",
    "                else:\n",
    "                    self.model[sentence[i-1]][sentence[i]]=1\n",
    "            else:\n",
    "                self.model[sentence[i-1]]={}\n",
    "                self.model[sentence[i-1]][sentence[i]]=1\n",
    "                \n",
    "    def cond_logprob_1(self, word_tuple, previous):\n",
    "        if word_tuple in self.model.keys():\n",
    "            return self.sample(self.model[word_tuple])\n",
    "        else:\n",
    "            return 'UNK'\n",
    "    \n",
    "    def cond_logprob(self, word_tuple, pick):\n",
    "        if word_tuple in self.model.keys():\n",
    "            D=self.model[word_tuple] \n",
    "            return D.get(pick,0)/sum(D.values())\n",
    "        else:\n",
    "            return self.lbackoff \n",
    "        \n",
    "    def sample(self, D):\n",
    "        values=np.array(list(D.values()))\n",
    "        prob= values/sum(D.values())\n",
    "        val=np.random.choice(len(D.keys()),1,p=prob)\n",
    "        return list(D.keys())[int(val)]\n",
    "\n",
    "class linear_interpolation(LangModel): \n",
    "    def __init__(self, gamma, t_model, b_model, u_model, modelled_unk=[]):\n",
    "        self.tri=t_model\n",
    "        self.bi=b_model\n",
    "        self.uni= u_model\n",
    "        self.gamma= gamma\n",
    "        self.modelled_unk=modelled_unk\n",
    "        self.lbackoff=0.0000001\n",
    "    def calc_lambdas(self, c_u_v, c_v):\n",
    "        l1=c_u_v/(c_u_v+ self.gamma)\n",
    "        l2=(1-l1)*c_v/(c_v+ self.gamma)\n",
    "        l3=1-l1-l2\n",
    "        return (l1,l2,l3)\n",
    "    \n",
    "    def perplexity(self, corpus):\n",
    "        \"\"\"Computes the perplexity of the corpus by the model.\n",
    "\n",
    "        Assumes the model uses an EOS symbol at the end of each sentence.\n",
    "        \"\"\"\n",
    "        return pow(2.0, self.entropy(corpus))\n",
    "\n",
    "    def entropy(self, corpus):\n",
    "        num_words = 0.0\n",
    "        sum_logprob = 0.0\n",
    "        for s in corpus:\n",
    "            num_words += len(s) + 1 # for EOS\n",
    "            sum_logprob += self.logprob_sentence_comb(s)\n",
    "        num_words=num_words-len(self.modelled_unk)+1\n",
    "        return -(1.0/num_words)*(sum_logprob)\n",
    "    \n",
    "    def logprob_sentence_comb(self, sentence):\n",
    "        p=0\n",
    "        sentence=['*','*'] + sentence + ['END_OF_SENTENCE']\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "            if (sentence[i] in self.modelled_unk) or (sentence[i] not in self.uni.model.keys()):\n",
    "                sentence[i]='UNK'\n",
    "                \n",
    "        for i in range(2, len(sentence)):\n",
    "            if (sentence[i-2], sentence[i-1]) in self.tri.model.keys():\n",
    "                c_u_v= sum(self.tri.model[(sentence[i-2], sentence[i-1])].values())\n",
    "            else:\n",
    "                c_u_v=0\n",
    "            if sentence[i-1] in self.bi.model.keys():\n",
    "                c_v=sum(self.bi.model[sentence[i-1]].values())\n",
    "            else:\n",
    "                c_v=0\n",
    "            l1,l2,l3=self.calc_lambdas(c_u_v, c_v)\n",
    "            temp=(l1*self.tri.cond_logprob((sentence[i-2], sentence[i-1]), sentence[i])+l2*self.bi.cond_logprob(sentence[i-1], sentence[i])+ l3*self.uni.model[sentence[i]]/sum(list(self.uni.model.values())))\n",
    "            p=p+math.log(temp,2)\n",
    "        return p\n",
    "    \n",
    "    def gen_next(self):\n",
    "        i=('*','*');sent=[]\n",
    "        while i[1]!='END_OF_SENTENCE':\n",
    "            sent.append(i[0])\n",
    "            if i in self.tri.model.keys():\n",
    "                c_u_v= sum(self.tri.model[i].values())\n",
    "            else:\n",
    "                c_u_v=0\n",
    "            if i[1] in self.bi.model.keys():\n",
    "                c_v=sum(self.bi.model[i[1]].values())\n",
    "            else:\n",
    "                c_v=0\n",
    "            l1,l2,l3=self.calc_lambdas(c_u_v, c_v)\n",
    "            a1=self.tri.cond_logprob_1(i, 0)\n",
    "            a2=self.bi.cond_logprob_1(i[1], 0)\n",
    "            x=np.random.choice(len(self.uni.model.keys()),1,p=np.array(list(self.uni.model.values()))/sum(self.uni.model.values()))\n",
    "            a3=list(self.uni.model.keys())[int(x)]\n",
    "            x= np.random.choice(3,1,p=[l1,l2,l3])\n",
    "            if x==0:\n",
    "                next_word=a1\n",
    "            elif x==1:\n",
    "                next_word=a2\n",
    "            else:\n",
    "                next_word=a3\n",
    "            i=(i[1],next_word)\n",
    "            s=' '.join(sent)\n",
    "        return s\n",
    "        \n",
    "trigram=Trigram(modelled_unk=modelled_unk)\n",
    "unigram=Unigram(modelled_unk=modelled_unk)\n",
    "bigram=Bigram(modelled_unk=modelled_unk)\n",
    "trigram.fit_corpus(data1.train+data2.train)\n",
    "bigram.fit_corpus(data1.train+data2.train)\n",
    "unigram.fit_corpus(data1.train+data2.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "L= linear_interpolation(1000, trigram, bigram, unigram, modelled_unk=modelled_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410.377261288678\n"
     ]
    }
   ],
   "source": [
    "print(L.perplexity(data2.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'* * According government after an increase of the plant located in the price on the first quarter when the company will result in five rural Main imposition slaughterhouse Japan would allocate in'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.gen_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51555.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2.train)*0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
